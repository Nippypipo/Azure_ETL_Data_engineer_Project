{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "086ba96c-d74e-48f4-9a98-923343f1e600",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import md5, concat_ws, col, current_date, lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14458f73-9d0d-4b2d-bd07-920bd822297b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "class MergePipeline:\n",
    "    def __init__(self, \n",
    "                 payload: str, \n",
    "                 run_id: str, \n",
    "                 adls_container_name: str = \"nipun\", \n",
    "                 adls_storage_account: str = \"delearningstdfssandbox\"):\n",
    "        self.spark = SparkSession.builder.appName(\"Merge Pipeline\").getOrCreate()\n",
    "        self.catalog_name = '`nipun-catalog`'\n",
    "        self.payload = json.loads(payload)\n",
    "        self.run_id = run_id.lower()\n",
    "        self.prefix_path = f\"abfss://{adls_container_name}@{adls_storage_account}.dfs.core.windows.net\"\n",
    "\n",
    "    def read_data(self):\n",
    "        staging_type = self.payload['StagingType']\n",
    "        \n",
    "        if staging_type == 'parquet':\n",
    "            return self.read_parquet()\n",
    "        elif staging_type == 'csv':\n",
    "            return self.read_csv()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported StagingType\")\n",
    "\n",
    "    def read_parquet(self):\n",
    "        folder_path_parquet = self.payload['Path'] + self.run_id\n",
    "        full_path = self.prefix_path + folder_path_parquet\n",
    "        return self.spark.read.parquet(full_path)\n",
    "\n",
    "    def read_csv(self):\n",
    "        folder_path_csv = f\"{self.payload['CsvPath']}{self.payload['DatasetName']}.csv\"\n",
    "        return self.spark.read.format(\"csv\").option(\"header\", \"true\").load(folder_path_csv)\n",
    "\n",
    "    def create_database(self):\n",
    "        schema_name = self.payload['SchemaName'] + \"_Nipun\"\n",
    "        create_db_query = f\"\"\"CREATE SCHEMA IF NOT EXISTS {self.catalog_name}.{schema_name}\"\"\"\n",
    "        self.spark.sql(create_db_query)\n",
    "        self.spark.sql(f\"USE {self.catalog_name}.{schema_name}\")\n",
    "\n",
    "    def create_table_if_not_exist(self, spark_df):\n",
    "        schema_name = self.payload['SchemaName'] + \"_Nipun\"\n",
    "        dataset_name = self.payload['DatasetName'].lower()\n",
    "\n",
    "        tables_df = self.spark.sql(\"SHOW TABLES\")\n",
    "        table_exists = tables_df.filter(tables_df.tableName == dataset_name).count() > 0\n",
    "\n",
    "        if not table_exists:\n",
    "            spark_df.write.format('delta').saveAsTable(f'{schema_name}.{dataset_name}')\n",
    "\n",
    "    def apply_update(self, spark_df):\n",
    "        update_type = self.payload.get('UpdateType', '')\n",
    "        load_type = self.payload['LoadType']\n",
    "        schema_name = self.payload['SchemaName'] + \"_Nipun\"\n",
    "        dataset_name = self.payload['DatasetName']\n",
    "        primary_key_fields = self.payload.get('PrimaryKeyFields', '')\n",
    "        \n",
    "        delta_table_path = f\"{self.prefix_path}/deltademo/{dataset_name.lower()}\"\n",
    "        delta_table = DeltaTable.forName(self.spark, f'{self.catalog_name}.{schema_name}.{dataset_name}')\n",
    "\n",
    "        invalid_values = [None, '', 'N/A', 'null']\n",
    "\n",
    "        def null_empty(value):\n",
    "            return value.strip() in invalid_values\n",
    "\n",
    "        def not_null_empty(value):\n",
    "            return value.strip() not in invalid_values\n",
    "\n",
    "        # SCD2 update\n",
    "        if update_type == \"scd2\":\n",
    "            return self.apply_scd2(spark_df)\n",
    "\n",
    "        # OP update\n",
    "        elif update_type == \"op\":\n",
    "            return self.apply_op(spark_df)\n",
    "\n",
    "        # Overwrite: replacing the existing data with new data.\n",
    "        elif load_type == \"F\":\n",
    "            return spark_df.write.format('delta').mode(\"overwrite\").saveAsTable(f'{schema_name}.{dataset_name}')\n",
    "\n",
    "        # Insert Only: adding new rows to the existing table without affecting the existing rows.\n",
    "        elif load_type == \"I\" and null_empty(primary_key_fields):\n",
    "            return spark_df.write.mode(\"append\").option('path', delta_table_path).saveAsTable(f\"{schema_name}.{dataset_name}\")\n",
    "\n",
    "        # Upsert: inserting new rows and updating existing rows based on a condition (usually the primary key)\n",
    "        elif load_type == \"I\" and not_null_empty(primary_key_fields):\n",
    "            def create_update_condition(primary_keys):\n",
    "                conditions = [f\"target.{key.strip()} = source.{key.strip()}\" for key in primary_keys.split(',')]\n",
    "                return ' AND '.join(conditions)\n",
    "\n",
    "            if ',' in primary_key_fields:\n",
    "                update_condition = create_update_condition(primary_key_fields)\n",
    "            else: \n",
    "                update_condition = f\"target.{primary_key_fields} = source.{primary_key_fields}\"\n",
    "\n",
    "            delta_table.alias(\"target\").merge(\n",
    "                spark_df.alias(\"source\"),\n",
    "                update_condition\n",
    "            ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid LoadType or UpdateType provided\")\n",
    "\n",
    "    def apply_scd2(self, spark_df):\n",
    "        schema_name = self.payload['SchemaName'] + \"_Nipun\"\n",
    "        scd_handler = SCDHandler(catalog_name=self.catalog_name, database_name=schema_name)\n",
    "        result_df = scd_handler.scd_2(\n",
    "            source_df=spark_df,\n",
    "            dataset_name=self.payload['DatasetName'],\n",
    "            database_name=schema_name,\n",
    "            catalog_name=self.catalog_name,\n",
    "            join_keys=[key.strip() for key in self.payload['PrimaryKeyFields'].split(',')]\n",
    "        )\n",
    "        result_df.write.format('delta').mode('overwrite').saveAsTable(f\"{self.catalog_name}.{schema_name}.{self.payload['DatasetName']}\")\n",
    "        return result_df\n",
    "\n",
    "    def apply_op(self, spark_df):\n",
    "        schema_name = self.payload['SchemaName'] + \"_Nipun\"\n",
    "        dataset_name = self.payload['DatasetName']\n",
    "        delta_table = DeltaTable.forName(self.spark, f'{self.catalog_name}.{schema_name}.{dataset_name}')\n",
    "        self.spark.sql(f\"DROP TABLE IF EXISTS {schema_name}.{dataset_name}\")\n",
    "\n",
    "        delta_table.alias('tgt').merge(\n",
    "            source=spark_df.alias('src'),\n",
    "            condition='\\nAND '.join([f'tgt.`{_col}` <=> src.`{_col}`' for _col in self.payload['PartitionFields']])\n",
    "        ).whenMatchedDelete().execute()\n",
    "\n",
    "        spark_df.write.format('delta').mode('append').saveAsTable(f'{self.catalog_name}.{schema_name}.{dataset_name}')\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        spark_df = self.read_data()\n",
    "        self.create_database()\n",
    "        self.create_table_if_not_exist(spark_df)\n",
    "        result_df = self.apply_update(spark_df)\n",
    "        result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cb8048a-7659-4a53-8ea5-ed7d02640c6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SCDHandler:\n",
    "    def __init__(self, catalog_name: str, database_name: str):\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"SCD2 Example\") \\\n",
    "            .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "            .enableHiveSupport() \\\n",
    "            .getOrCreate()\n",
    "        self.spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "        self.spark.sql(f\"USE {database_name}\")\n",
    "\n",
    "    def check_columns_presence(self, source_df, target_df, metadata_cols):\n",
    "        cols_missing = set([cols for cols in target_df.columns if cols not in source_df.columns]) - set(metadata_cols)\n",
    "        if cols_missing:\n",
    "            raise Exception(f\"Cols missing in source DataFrame: {cols_missing}\")\n",
    "\n",
    "    def apply_hash_and_alias(self, source_df, target_df, metadata_cols):\n",
    "        tgt_cols = [x for x in target_df.columns if x not in metadata_cols]\n",
    "        hash_expr = md5(concat_ws(\"|\", *[col(c) for c in tgt_cols]))\n",
    "        source_df = source_df.withColumn(\"hash_value\", hash_expr).alias(\"source_df\")\n",
    "        target_df = target_df.withColumn(\"hash_value\", hash_expr).alias(\"target_df\")\n",
    "        return source_df, target_df\n",
    "\n",
    "    def scd_2(self, source_df, dataset_name, database_name, catalog_name, join_keys, metadata_cols=None):\n",
    "        if metadata_cols is None:\n",
    "            metadata_cols = ['eff_start_date', 'eff_end_date', 'flag']\n",
    "\n",
    "        target_df = DeltaTable.forName(self.spark, f'{catalog_name}.{database_name}.{dataset_name}').toDF()\n",
    "        self.check_columns_presence(source_df, target_df, metadata_cols)\n",
    "\n",
    "        source_df, target_df = self.apply_hash_and_alias(source_df, target_df, metadata_cols)\n",
    "\n",
    "        join_cond = [source_df[join_key] == target_df[join_key] for join_key in join_keys]\n",
    "        new_df = source_df.join(target_df, join_cond, 'left_anti')\n",
    "\n",
    "        base_df = target_df.join(source_df, join_cond, 'left')\n",
    "        unchanged_filter_expr = \" AND \".join([f\"source_df.{key} IS NULL\" for key in join_keys])\n",
    "        unchanged_df = base_df.filter(f\"({unchanged_filter_expr}) OR (source_df.hash_value = target_df.hash_value)\") \\\n",
    "            .select(\"target_df.*\")\n",
    "\n",
    "        delta_filter_expr = \" and \".join([f\"source_df.{key} IS NOT NULL\" for key in join_keys])\n",
    "        updated_df = base_df.filter(f\"{delta_filter_expr} AND source_df.hash_value != target_df.hash_value\")\n",
    "        updated_new_df = updated_df.select(\"source_df.*\")\n",
    "        obsolete_df = updated_df.select(\"target_df.*\") \\\n",
    "            .withColumn(\"eff_end_date\", current_date()) \\\n",
    "            .withColumn(\"flag\", lit(0))\n",
    "\n",
    "        delta_df = new_df.union(updated_new_df) \\\n",
    "            .withColumn(\"eff_start_date\", current_date()) \\\n",
    "            .withColumn(\"eff_end_date\", lit(None)) \\\n",
    "            .withColumn(\"flag\", lit(1))\n",
    "\n",
    "        # Drop the 'hash_value' column from the result_df\n",
    "        result_df = unchanged_df.select(target_df.columns). \\\n",
    "            unionByName(delta_df.select(target_df.columns)). \\\n",
    "            unionByName(obsolete_df.select(target_df.columns)). \\\n",
    "            drop(\"hash_value\")\n",
    "\n",
    "    \n",
    "        return result_df\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "merge_functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
