{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0640056-6d41-4169-9567-33642c10d904",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Import module from other notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54cb296a-d8a6-4b4c-9d5e-8ed142371e28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/nipun.a@bluebik.com/de-learning/src/databricks/merge_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be7d27da-89c2-48fc-bc14-7d8e2617e7c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install nutter --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6be8a0-9e5b-40f9-82a1-b6335d53b4fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from databricks.sdk.runtime import *\n",
    "from delta.tables import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from pyspark.sql.functions import col, lit, current_date, md5, concat_ws\n",
    "from delta.tables import DeltaTable\n",
    "from runtime.nutterfixture import NutterFixture, tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2467428f-1621-4873-a759-d02a922b0b42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Unit testing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51510513-b456-4182-b92f-f6d6029890ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TestMergePipeline(NutterFixture):\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Test Merge Pipeline\").getOrCreate()\n",
    "        self.PayLoadParquet = \"\"\"{\n",
    "                            \"Path\": \"/raw/sql_server/ShippedOrders/year=2024/month=7/day=31/run_id=\",\n",
    "                            \"LocalRunDatetime\": \"Jul 23 2024  5:25PM\",\n",
    "                            \"DatasetName\": \"test_dataset\",\n",
    "                            \"SchemaName\": \"test_schema\",\n",
    "                            \"LoadType\": \"I\",\n",
    "                            \"PrimaryKeyFields\": \"order_id\",\n",
    "                            \"StagingType\": \"parquet\",\n",
    "                            \"PartitionFields\": [\"year\"],\n",
    "                            \"UpdateType\": \"\"\n",
    "                        }\"\"\"\n",
    "        self.RunIdParquet = \"5b408abd-ec21-4807-8eaf-dff9b17ee6dc\"\n",
    "        self.CatalogName = '`nipun-catalog`'\n",
    "        self.PayLoadCsv =   \"\"\"{\n",
    "                    \"LocalRunDatetime\": \"Aug  7 2024  2:41PM\",\n",
    "                    \"DatasetName\": \"SalesOrderHeader\",\n",
    "                    \"LoadType\": \"F\",\n",
    "                    \"UpdateType\": null,\n",
    "                    \"PrimaryKeyFields\": null,\n",
    "                    \"CsvPath\": \"abfss://nipun@delearningstdfssandbox.dfs.core.windows.net/raw_csv/\",\n",
    "                    \"CsvDelimiter\": \"','\",\n",
    "                    \"StagingType\": \"csv\"\n",
    "                }\n",
    "                \"\"\"\n",
    "        self.RunIdCsv = \"E4D1E20D-9F43-4CB1-A2B2-2E244A7C2F62\"\n",
    "        super().__init__() \n",
    "\n",
    "    def assertion_read_data_csv(self):\n",
    "        etl_pipeline = MergePipeline(payload=self.PayLoadCsv, run_id=self.RunIdCsv)\n",
    "        df_actual = etl_pipeline.read_data()\n",
    "        df_expect = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"abfss://nipun@delearningstdfssandbox.dfs.core.windows.net/raw_csv/SalesOrderHeader.csv\")\n",
    "\n",
    "        df_diff = df_actual.subtract(df_expect).union(df_expect.subtract(df_actual))\n",
    "        assert df_diff.count() == 0, \"DataFrames do not match\"\n",
    "    \n",
    "    def assertion_read_data_parquet(self):\n",
    "        etl_pipeline = MergePipeline(payload=self.PayLoadParquet, run_id=self.RunIdParquet)\n",
    "        df_actual = etl_pipeline.read_data()\n",
    "        df_expect = spark.read.parquet(\"abfss://nipun@delearningstdfssandbox.dfs.core.windows.net/raw/sql_server/ShippedOrders/year=2024/month=7/day=31/run_id=5b408abd-ec21-4807-8eaf-dff9b17ee6dc\")\n",
    "\n",
    "        df_diff = df_actual.subtract(df_expect).union(df_expect.subtract(df_actual))\n",
    "        assert df_diff.count() == 0, \"DataFrames do not match\"\n",
    "\n",
    "    def assertion_create_database(self):\n",
    "        etl_pipeline = MergePipeline(self.PayLoadParquet, run_id=self.RunIdParquet)\n",
    "        etl_pipeline.create_database()\n",
    "\n",
    "        use_catalog = f\"USE CATALOG {self.CatalogName}\"\n",
    "        self.spark.sql(use_catalog)\n",
    "\n",
    "        schema_name = \"test_schema_Nipun\"\n",
    "        schema_exists = self.spark.sql(f\"SHOW SCHEMAS LIKE '{schema_name}'\").count() > 0\n",
    "        assert schema_exists, f\"Schema {schema_name} was not created successfully\"\n",
    "\n",
    "        drop_schema = f\"DROP SCHEMA IF EXISTS {self.CatalogName}.{schema_name} CASCADE\"\n",
    "        self.spark.sql(drop_schema)\n",
    "        \n",
    "        schema_exists_after_cleanup = self.spark.sql(f\"SHOW SCHEMAS LIKE '{schema_name}'\").count() == 0\n",
    "        assert schema_exists_after_cleanup, f\"Schema {schema_name} was not removed after cleanup\"\n",
    "  \n",
    "    def assertion_create_table_if_not_exist(self):\n",
    "        json_object = json.loads(self.PayLoadParquet)\n",
    "        schema_name = json_object['SchemaName'] + \"_Nipun\"\n",
    "        dataset_name = json_object['DatasetName'].lower()\n",
    "\n",
    "        mock_data = [(1, \"sample_data\")]\n",
    "        spark_df = self.spark.createDataFrame(mock_data, [\"id\", \"value\"])\n",
    "\n",
    "        etl_pipeline = MergePipeline(payload=self.PayLoadParquet, run_id=self.RunIdParquet)\n",
    "        etl_pipeline.create_database()\n",
    "        etl_pipeline.create_table_if_not_exist(spark_df)\n",
    "\n",
    "        use_catalog = f\"USE CATALOG {self.CatalogName}\"\n",
    "        self.spark.sql(use_catalog)\n",
    "        show_tables_query = f\"SHOW TABLES IN {self.CatalogName}.{schema_name}\"\n",
    "        table_exists_after = self.spark.sql(show_tables_query).filter(f\"tableName = '{dataset_name}'\").count() > 0\n",
    "        assert table_exists_after, f\"Table {dataset_name} was not created successfully\"\n",
    "\n",
    "        drop_schema = f\"DROP SCHEMA IF EXISTS {self.CatalogName}.{schema_name} CASCADE\"\n",
    "        self.spark.sql(drop_schema)\n",
    "\n",
    "    def assertion_apply_full_load(self):\n",
    "        json_object = json.loads(self.PayLoadParquet)\n",
    "        schema_name = json_object['SchemaName'] + \"_Nipun\"\n",
    "        dataset_name = json_object['DatasetName'].lower()\n",
    "\n",
    "        initial_data = [(1, \"initial_data_1\"), (2, \"initial_data_2\")]\n",
    "        initial_df = self.spark.createDataFrame(initial_data, [\"id\", \"value\"])\n",
    "\n",
    "        create_schema = f\"\"\"CREATE SCHEMA IF NOT EXISTS {self.CatalogName}.{schema_name}\"\"\"\n",
    "        self.spark.sql(create_schema)\n",
    "        initial_df.write.format('delta').mode(\"overwrite\").saveAsTable(f'{schema_name}.{dataset_name}')\n",
    "\n",
    "        table_data_before = self.spark.sql(f\"SELECT * FROM {schema_name}.{dataset_name}\")\n",
    "        assert table_data_before.count() == 2, \"Initial data not written correctly to the table\"\n",
    "\n",
    "        new_data = [(3, \"new_data_1\"), (4, \"new_data_2\")]\n",
    "        new_df = self.spark.createDataFrame(new_data, [\"id\", \"value\"])\n",
    "\n",
    "        json_object['LoadType'] = \"F\"\n",
    "        self.PayLoadParquet = json.dumps(json_object)\n",
    "        \n",
    "        merge_pipeline = MergePipeline(payload=self.PayLoadParquet, run_id=self.RunIdParquet)\n",
    "        merge_pipeline.apply_update(new_df)\n",
    "\n",
    "        table_data_after = self.spark.sql(f\"SELECT * FROM {schema_name}.{dataset_name}\")\n",
    "        assert table_data_after.count() == 2, \"Table was not overwritten correctly\"\n",
    "        assert table_data_after.filter(\"id = 3\").count() == 1, \"New data was not loaded correctly\"\n",
    "        assert table_data_after.filter(\"id = 1\").count() == 0, \"Old data was not overwritten correctly\"\n",
    "\n",
    "        drop_schema = f\"DROP SCHEMA IF EXISTS {self.CatalogName}.{schema_name} CASCADE\"\n",
    "        self.spark.sql(drop_schema)\n",
    "\n",
    "    def assertion_apply_insert_load(self):\n",
    "        json_object = json.loads(self.PayLoadParquet)\n",
    "        schema_name = json_object['SchemaName'] + \"_Nipun\"\n",
    "        dataset_name = json_object['DatasetName'].lower()\n",
    "\n",
    "        initial_data = [(1, \"initial_data_1\"), (2, \"initial_data_2\")]\n",
    "        initial_df = self.spark.createDataFrame(initial_data, [\"id\", \"value\"])\n",
    "\n",
    "        create_schema = f\"\"\"CREATE SCHEMA IF NOT EXISTS {self.CatalogName}.{schema_name}\"\"\"\n",
    "        self.spark.sql(create_schema)\n",
    "        initial_df.write.format('delta').mode(\"overwrite\").saveAsTable(f'{schema_name}.{dataset_name}')\n",
    "\n",
    "        table_data_before = self.spark.sql(f\"SELECT * FROM {schema_name}.{dataset_name}\")\n",
    "        assert table_data_before.count() == 2, \"Initial data not written correctly to the table\"\n",
    "\n",
    "        new_data = [(3, \"new_data_1\"), (4, \"new_data_2\")]\n",
    "        new_df = self.spark.createDataFrame(new_data, [\"id\", \"value\"])\n",
    "\n",
    "        json_object['LoadType'] = \"I\"\n",
    "        json_object['PrimaryKeyFields'] = \"\"  \n",
    "        self.PayLoadParquet = json.dumps(json_object)\n",
    "\n",
    "        merge_pipeline = MergePipeline(payload=self.PayLoadParquet, run_id=self.RunIdParquet)\n",
    "        merge_pipeline.apply_update(new_df)\n",
    "\n",
    "        table_data_after = self.spark.sql(f\"SELECT * FROM {schema_name}.{dataset_name}\")\n",
    "        assert table_data_after.count() == 4, \"Data was not inserted correctly\"\n",
    "        assert table_data_after.filter(\"id = 3\").count() == 1, \"New data was not loaded correctly\"\n",
    "        assert table_data_after.filter(\"id = 1\").count() == 1, \"Existing data was overwritten, which should not happen\"\n",
    "\n",
    "        drop_schema = f\"DROP SCHEMA IF EXISTS {self.CatalogName}.{schema_name} CASCADE\"\n",
    "        self.spark.sql(drop_schema)\n",
    "\n",
    "    def assertion_apply_upsert_load(self):\n",
    "        json_object = json.loads(self.PayLoadParquet)\n",
    "        schema_name = json_object['SchemaName'] + \"_Nipun\"\n",
    "        dataset_name = json_object['DatasetName'].lower()\n",
    "\n",
    "        create_schema = f\"\"\"CREATE SCHEMA IF NOT EXISTS {self.CatalogName}.{schema_name}\"\"\"\n",
    "        self.spark.sql(create_schema)\n",
    "\n",
    "        initial_data = [(1, \"initial_data_1\"), (2, \"initial_data_2\")]\n",
    "        initial_df = self.spark.createDataFrame(initial_data, [\"id\", \"value\"])\n",
    "        initial_df.write.format('delta').mode(\"overwrite\").saveAsTable(f'{schema_name}.{dataset_name}')\n",
    "\n",
    "        table_data_before = self.spark.sql(f\"SELECT * FROM {schema_name}.{dataset_name}\")\n",
    "        assert table_data_before.count() == 2, \"Initial data not written correctly to the table\"\n",
    "\n",
    "        upsert_data = [(2, \"updated_data_2\"), (3, \"new_data_3\")]\n",
    "        upsert_df = self.spark.createDataFrame(upsert_data, [\"id\", \"value\"])\n",
    "\n",
    "        json_object['LoadType'] = \"I\"\n",
    "        json_object['PrimaryKeyFields'] = \"id\"  \n",
    "        self.PayLoadParquet = json.dumps(json_object)\n",
    "\n",
    "        merge_pipeline = MergePipeline(payload=self.PayLoadParquet, run_id=self.RunIdParquet)\n",
    "        merge_pipeline.apply_update(upsert_df)\n",
    "\n",
    "        table_data_after = self.spark.sql(f\"SELECT * FROM {schema_name}.{dataset_name}\")\n",
    "        assert table_data_after.count() == 3, \"Data was not upserted correctly\"\n",
    "        \n",
    "        assert table_data_after.filter(\"id = 2 AND value = 'updated_data_2'\").count() == 1, \"Existing data was not updated correctly\"\n",
    "        assert table_data_after.filter(\"id = 3 AND value = 'new_data_3'\").count() == 1, \"New data was not inserted correctly\"\n",
    "\n",
    "        drop_schema = f\"DROP SCHEMA IF EXISTS {self.CatalogName}.{schema_name} CASCADE\"\n",
    "        self.spark.sql(drop_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a80b23-95a2-49ba-a742-a0d9a7dac5d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TestMergePipeline2(NutterFixture):\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Test Merge Pipeline\").getOrCreate()\n",
    "        self.PayLoadParquet = \"\"\"{\n",
    "                            \"Path\": \"/raw/sql_server/ShippedOrders/year=2024/month=7/day=31/run_id=\",\n",
    "                            \"LocalRunDatetime\": \"Jul 23 2024  5:25PM\",\n",
    "                            \"DatasetName\": \"test_dataset\",\n",
    "                            \"SchemaName\": \"test_schema\",\n",
    "                            \"LoadType\": \"I\",\n",
    "                            \"PrimaryKeyFields\": \"order_id\",\n",
    "                            \"StagingType\": \"parquet\",\n",
    "                            \"PartitionFields\": [\"year\"],\n",
    "                            \"UpdateType\": \"\"\n",
    "                        }\"\"\"\n",
    "        self.RunIdParquet = \"5b408abd-ec21-4807-8eaf-dff9b17ee6dc\"\n",
    "        self.CatalogName = '`nipun-catalog`'\n",
    "        self.PayLoadCsv =   \"\"\"{\n",
    "                    \"LocalRunDatetime\": \"Aug  7 2024  2:41PM\",\n",
    "                    \"DatasetName\": \"SalesOrderHeader\",\n",
    "                    \"LoadType\": \"F\",\n",
    "                    \"UpdateType\": null,\n",
    "                    \"PrimaryKeyFields\": null,\n",
    "                    \"CsvPath\": \"abfss://nipun@delearningstdfssandbox.dfs.core.windows.net/raw_csv/\",\n",
    "                    \"CsvDelimiter\": \"','\",\n",
    "                    \"StagingType\": \"csv\"\n",
    "                }\n",
    "                \"\"\"\n",
    "        self.RunIdCsv = \"E4D1E20D-9F43-4CB1-A2B2-2E244A7C2F62\"\n",
    "        super().__init__()\n",
    "\n",
    "    def assertion_apply_op(self):\n",
    "        json_object = json.loads(self.PayLoadParquet)\n",
    "        schema_name = json_object['SchemaName'] + \"_Nipun\"\n",
    "        dataset_name = json_object['DatasetName'].lower()\n",
    "\n",
    "        # Create a DataFrame with initial data to simulate existing records in the table\n",
    "        initial_data = [\n",
    "            (2020, 'USA', 1000),\n",
    "            (2020, 'Canada', 1500),\n",
    "            (2020, 'UK', 2000),\n",
    "            (2021, 'USA', 2000),\n",
    "            (2021, 'Canada', 3500),\n",
    "            (2021, 'UK', 1000),\n",
    "        ]\n",
    "        initial_schema = [\"year\", \"country\", \"amount\"]\n",
    "        initial_df = self.spark.createDataFrame(initial_data, initial_schema)\n",
    "\n",
    "        create_schema = f\"\"\"CREATE SCHEMA IF NOT EXISTS {self.CatalogName}.{schema_name}\"\"\"\n",
    "        self.spark.sql(create_schema)\n",
    "        initial_df.write.format('delta').mode('overwrite').saveAsTable(f\"{self.CatalogName}.{schema_name}.{dataset_name}\")\n",
    "\n",
    "        new_data = [\n",
    "            (2020, 'USA', 1112),\n",
    "            (2020, 'Canada', 4445),\n",
    "            (2022, 'USA', 5667),\n",
    "            (2022, 'Canada', 3244),\n",
    "            (2022, 'UK', 8976),\n",
    "        ]\n",
    "        new_schema = [\"year\", \"country\", \"amount\"]\n",
    "        new_df = self.spark.createDataFrame(new_data, new_schema)\n",
    "\n",
    "        merge_pipeline = MergePipeline(payload=self.PayLoadParquet, run_id=self.RunIdParquet)\n",
    "        merge_pipeline.apply_op(new_df)\n",
    "\n",
    "        query_table = f\"SELECT * FROM {self.CatalogName}.{schema_name}.{dataset_name}\"\n",
    "        final_table_df = self.spark.sql(query_table)\n",
    "        #final_table_df.show(truncate=False)  # Optional: display the result for debugging\n",
    "\n",
    "        expected_data = [\n",
    "            Row(year=2020, country='Canada', amount=4445),\n",
    "            Row(year=2020, country='USA', amount=1112),\n",
    "            Row(year=2021, country='Canada', amount=3500),\n",
    "            Row(year=2021, country='UK', amount=1000),\n",
    "            Row(year=2021, country='USA', amount=2000),\n",
    "            Row(year=2022, country='Canada', amount=3244),\n",
    "            Row(year=2022, country='UK', amount=8976),\n",
    "            Row(year=2022, country='USA', amount=5667),\n",
    "        ]\n",
    "        expected_df = self.spark.createDataFrame(expected_data)\n",
    "\n",
    "        assert final_table_df.subtract(expected_df).count() == 0, \"The table data does not match the expected result\"\n",
    "\n",
    "        drop_schema = f\"DROP SCHEMA IF EXISTS {self.CatalogName}.{schema_name} CASCADE\"\n",
    "        self.spark.sql(drop_schema)\n",
    "\n",
    "    def compare_dataframes(self, df1: DataFrame, df2: DataFrame) -> bool:\n",
    "        if df1.schema != df2.schema:\n",
    "            print(\"Schemas are different\")\n",
    "            return False\n",
    "\n",
    "        if df1.count() != df2.count():\n",
    "            print(\"Row counts are different\")\n",
    "            return False\n",
    "\n",
    "        df1_sorted = df1.orderBy(df1.columns)\n",
    "        df2_sorted = df2.orderBy(df2.columns)\n",
    "\n",
    "        df1_list = df1_sorted.collect()\n",
    "        df2_list = df2_sorted.collect()\n",
    "\n",
    "        if df1_list != df2_list:\n",
    "            print(\"Data is different\")\n",
    "            return False\n",
    "\n",
    "        print(\"DataFrames are identical\")\n",
    "        return True\n",
    "\n",
    "    def assertion_apply_scd2_load(self):\n",
    "        json_object = json.loads(self.PayLoadParquet)\n",
    "        schema_name = json_object['SchemaName'] + \"_Nipun\"\n",
    "        dataset_name = json_object['DatasetName'].lower()\n",
    "\n",
    "        schema = StructType([\n",
    "                    StructField(\"CustomerID\", IntegerType(), True),\n",
    "                    StructField(\"Name\", StringType(), True),\n",
    "                    StructField(\"Region\", StringType(), True),\n",
    "                    StructField(\"eff_start_date\", DateType(), True),\n",
    "                    StructField(\"eff_end_date\", DateType(), True),\n",
    "                    StructField(\"flag\", IntegerType(), True)\n",
    "                ])\n",
    "        data_target = [\n",
    "            (1, \"John Doe\", \"North\", datetime.strptime(\"2023-01-01\", \"%Y-%m-%d\"), None, 1),\n",
    "            (2, \"Jane Smith\", \"South\", datetime.strptime(\"2023-01-01\", \"%Y-%m-%d\"), None, 1),\n",
    "            (3, \"Mike Johnson\", \"East\", datetime.strptime(\"2023-01-01\", \"%Y-%m-%d\"), None, 1)\n",
    "        ]\n",
    "\n",
    "        df_target = spark.createDataFrame(data_target, schema)   \n",
    "        create_schema = f\"\"\"CREATE SCHEMA IF NOT EXISTS {self.CatalogName}.{schema_name}\"\"\"          \n",
    "        self.spark.sql(create_schema)\n",
    "        df_target.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{schema_name}.{dataset_name}\")\n",
    "\n",
    "        source_schema = StructType([\n",
    "            StructField(\"CustomerID\", IntegerType(), True),\n",
    "            StructField(\"Name\", StringType(), True),\n",
    "            StructField(\"Region\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "        data_source = [\n",
    "            (1, \"John Doe\", \"West\"),\n",
    "            (4, \"Alice Brown\", \"North\")  \n",
    "        ]\n",
    "\n",
    "        df_source = spark.createDataFrame(data_source, schema=source_schema)\n",
    "\n",
    "        json_object['UpdateType'] = \"scd2\"\n",
    "        json_object['PrimaryKeyFields'] = \"CustomerID\"\n",
    "        self.PayLoadParquet = json.dumps(json_object)\n",
    "\n",
    "        merge_pipeline = MergePipeline(payload=self.PayLoadParquet, run_id=\"mock_run_id\")\n",
    "        merge_pipeline.apply_update(df_source)\n",
    "\n",
    "        actual_df = DeltaTable.forName(self.spark, f'{self.CatalogName}.{schema_name}.{dataset_name}').toDF()\n",
    "\n",
    "        expected_data = [\n",
    "            (1, \"John Doe\", \"North\", datetime.strptime(\"2023-01-01\", \"%Y-%m-%d\").date(), datetime.today().date(), 0),\n",
    "            (3, \"Mike Johnson\", \"East\", datetime.strptime(\"2023-01-01\", \"%Y-%m-%d\").date(), None, 1),\n",
    "            (4, \"Alice Brown\", \"North\", datetime.today().date(), None, 1),\n",
    "            (2, \"Jane Smith\", \"South\", datetime.strptime(\"2023-01-01\", \"%Y-%m-%d\").date(), None, 1),\n",
    "            (1, \"John Doe\", \"West\",datetime.today().date(), None, 1)\n",
    "        ]\n",
    "\n",
    "        expected_df = spark.createDataFrame(expected_data, schema) \n",
    "\n",
    "        assert self.compare_dataframes(actual_df, expected_df), \"SCD2 update failed\"\n",
    "\n",
    "        drop_schema = f\"DROP SCHEMA IF EXISTS {self.CatalogName}.{schema_name} CASCADE\"\n",
    "        self.spark.sql(drop_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b134aa6a-7839-4b45-957d-126e73d892a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Run Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6f3982a-246f-4532-a1f5-3068833acdad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nNotebook: N/A - Lifecycle State: N/A, Result: N/A\nRun Page URL: N/A\n============================================================\nPASSING TESTS\n------------------------------------------------------------\napply_full_load (11.708774161999827 seconds)\napply_insert_load (7.2016212669987 seconds)\napply_upsert_load (8.904465669000274 seconds)\ncreate_database (1.4783431269988796 seconds)\ncreate_table_if_not_exist (4.525577089998478 seconds)\nread_data_csv (1.0168841579998116 seconds)\nread_data_parquet (1.0076113070008432 seconds)\n\n\n============================================================\n\n"
     ]
    }
   ],
   "source": [
    "result = TestMergePipeline().execute_tests()\n",
    "print(result.to_string())\n",
    "\n",
    "# Comment out the next line (result.exit(dbutils)) to see the test result report from within the notebook\n",
    "#result.exit(dbutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66185c68-b119-41dc-bdc6-0e8970d26a32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames are identical\n\nNotebook: N/A - Lifecycle State: N/A, Result: N/A\nRun Page URL: N/A\n============================================================\nPASSING TESTS\n------------------------------------------------------------\napply_op (11.445538974001465 seconds)\napply_scd2_load (10.676581640000222 seconds)\n\n\n============================================================\n\n"
     ]
    }
   ],
   "source": [
    "result = TestMergePipeline2().execute_tests()\n",
    "print(result.to_string())\n",
    "\n",
    "# Comment out the next line (result.exit(dbutils)) to see the test result report from within the notebook\n",
    "#result.exit(dbutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "564fe2ca-ce39-4964-a871-cf59f9dac790",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CatalogName = '`nipun-catalog`'\n",
    "schema_name = 'test_schema_Nipun'\n",
    "drop_schema = f\"DROP SCHEMA IF EXISTS {CatalogName}.{schema_name} CASCADE\"\n",
    "spark.sql(drop_schema)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "test_merge_functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
